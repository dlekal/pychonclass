# import numpy as np
#
# def sigmoid(x):
#     return 1 / (1 + np.exp(-x))
#
# def softmax(x):
#     # overflow 방지를 위해 입력에서 최대값을 빼줍니다.
#     enp_x = np.exp(x - np.max(x))
#     return enp_x / np.sum(enp_x, axis = 0)
#
# x = np.array([1.0, 2.0, 3.0])
#
# print("softmax 결과:", softmax(x))
#
# def relu(x):
#     return np.maxumum(0, x) # leaky relu면 (0.01x, x)
#
# # x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])
#
# def tanh(x):
#     return np.tanh(x)
#
# # x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])
#
# print("tanh 결과:", tanh(x))
#
# def binary_cross_entrorpy(y_true, y_pred):
#     epsilon = 1e-15
#     y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
#     return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
#
# y_true = np.array([1, 0, 1])
# y_pred = np.array([0.9, 0.2, 0.8])
# print("bce 손실:", binary_cross_entrorpy(y_true,y_pred))
#
#
# def mean_squared_error(y_true, y_pred):
#     return np.mean((y_true - y_pred) ** 2)
#
# y_true = np.array([1, 0, 1])
# y_pred = [0.9, 0.2,0.8]
# print("mse 손실:",mean_squared_error(y_true,y_pred))
#
# def categorical_cross_entropy(y_true, y_pred):
#
#     epsilon = 1e-15
#     y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
#     return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]
#
# y_true = np.array([[1,0,0], [0,1,0], [0,0,1]])
# y_pred = np.array([[0.9, 0.05, 0.05], [0.1, 0.8, 0.1], [0.05, 0.1, 0.85]])

# 활성화 함수는 넘길지 말지 결정하고 넘긴다면 얼마나 넘길지
# 손실함수는 예측한 정답을 실제 정답과 얼마나 차이나는지 계산

# import numpy as np
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense
# from tensorflow.keras.optimizers import SGD
#
# # 1. xor 데이터셋 정의
# # xor 문제를 위한 입력과 출력 정의
# # 입력: 2차원 배열(두 개의 이진 값)
# # 출력: xor 연산 결과 (0 또는 1)
# x = np.array([[0,0] , [0,1], [1,0], [1,1]]) # 입력 (AND, OR 등을 통해 연산 가능)
# y = np.array([[0], [1], [1], [0]])          # XOR 결과 (출력)
#
#
# # 2. 신경망 모델 정의
# model = Sequential()
#
#
# # 입력 레이어 및 은닉층 추가 (은닉층 2개, 노드 8개)
# model.add(Dense(8, input_dim = 2, activation= 'sigmoid')) # 은닉층 1개
# model.add(Dense(1, activation= 'sigmoid')) # 출려층 (이진 분류)
#
#
# # 3. 모델 컴파일 (손실 함수와 옵티마이저 정의)
# model.compile(loss = 'binary_crossentropy', optimizer = SGD(learning_rate=0.1), metrics = ['accuracy'])
#
#
# # 4. 모델 학습
# model.fit(x, y, epochs = 5000, verbose = 0)
#
#
# # 5. 학습 결과 테스트
# # 새로운 입력 데이터를 넣어 XOR 결과 예측
# predictions = model.predict(x)
# print("XOR 문제 예측 결과:")
# for i in range(len(x)):
#     print(f"입력: {x[i]} -> 에측 출력: {round(predictions[i][0])}, 실제 출력: {y[i][0]}")
#
# 노트북은 내장 그래픽을 사용함 그래서 코랩이라는 사이트에서 gpu 사용을 한다.
# 역전파란 신경망에서 오차를 계산하고 이를 바탕으로 가중치를 조정한다.

import torch
import torch.nn as nn
import torch.optim as optim

# XOR 데이터 셋

x_train = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype= torch.float32)
y_train = torch.tensor([[0],[1], [1], [0]], dtype=torch.float32)

# 신경망 정의 (2층 신경망)

class XORModel(nn.Module):
    def __init__(self):
        super(XORModel, self).__init__()
        self.layer1 = nn.Linear(2, 2) # 입력 2개 -> 은닉층 2개
        self.layer2 = nn.Linear(2, 1) # 은닉층 2개 -> 출력 1개
        self.sigmoid = nn.Sigmoid()   # sigmoid 활성화 함수

    def forward(self, x):
        out =  self.sigmoid(self.layer1(x)) # 은닉층 활성화
        out = self.sigmoid(self.layer2(out)) # 출력층 활성화
        return out

# 모델과 옵티마이저 정의 (SGD)

model = XORModel()
optimizer = optim.SGD(model.parameters(), lr=0.1) # 기본 SGD

# 학습
def train_model(model, optimizer, epochs=10000):
    criterion = nn.MSELoss()
    for epoch in range(epochs):
        y_pred = model(x_train)
        loss = criterion(y_pred, y_train)

        optimizer.zero_grad() # 기울기 초기화
        loss.backward() # 역전파
        optimizer.step() # 가중치 업데이트

        if epoch % 1000 == 0:
            print(f'Epoch {epoch}, loss: {loss.item():.4f}')
print("Training with SGD")
train_model(model, optimizer)

